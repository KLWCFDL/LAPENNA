{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdB6AqY__PXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ae7f5b-0fd1-4a2d-d658-23dacbecd109"
      },
      "source": [
        "# the code for updating the parameters in question 2\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "x = tf.constant([-0.04, -0.42],shape=[1, 2],dtype=tf.float32) # sets the input [X1, X2]\n",
        "\n",
        "w1 = tf.constant([[-2.5, -1.5],[0.6, 0.4]]) #setting the weight matrix for first fully connected layer [w1, w2];[w3, w4]\n",
        "w2 = tf.constant([[-0.1, 2.4, -2.2],[-1.5, -5.2, 3.7]]) #setting the weight matrix for second fc layer [w5, w6, w7];[w8, w9, w10]\n",
        "\n",
        "t = tf.constant([[0, 1, 0]], dtype=tf.float32) # sets the target matrix [t1, t2, t3]\n",
        "\n",
        "b1 = tf.constant([-1.6, 0.7]) #sets the bias1.\n",
        "b2 = tf.constant([0, 0, 1], dtype=tf.float32) #sets the bias2.\n",
        "\n",
        "def forward(x,w1,w2,b1,b2):\n",
        "    sum1=tf.matmul(x,w1)+b1\n",
        "    H = tf.nn.sigmoid(sum1)\n",
        "    print(H)\n",
        "    sum2=tf.matmul(H,w2)+b2\n",
        "    O = tf.nn.softmax(sum2)\n",
        "    return O\n",
        "\n",
        "O = forward(x, w1, w2, b1, b2)\n",
        "\n",
        "print('Forward path output is:', O.numpy())\n",
        "\n",
        "def backward(x,w1,w2,b1,b2,t):\n",
        "    with tf.GradientTape() as g:\n",
        "       g.watch([w1,w2,x,b1,b2])\n",
        "       O = forward(x, w1, w2, b1, b2)\n",
        "       Etotal = tf.losses.categorical_crossentropy(t, O)\n",
        "       print(\"Total loss = \" , Etotal.numpy())\n",
        "    dEtotal_dw = g.gradient(Etotal, [w1,w2,x,b1,b2]) #calculates the gradients for each part\n",
        "    dEtotal_dw1, dEtotal_dw2, dEtotal_dx, dEtotal_db1, dEtotal_db2 = dEtotal_dw\n",
        "    return dEtotal_dw1,dEtotal_dw2,dEtotal_dx,dEtotal_db1,dEtotal_db2\n",
        "\n",
        "dEtotal_dw1,dEtotal_dw2,dEtotal_dx,dEtotal_db1,dEtotal_db2 = backward(x,w1,w2,b1,b2,t)\n",
        "\n",
        "\n",
        "w1 = w1 - 0.5*dEtotal_dw1 #the learning rate is set to be 0.5\n",
        "w2 = w2 - 0.5*dEtotal_dw2\n",
        "\n",
        "b1 = b1 - 0.5*dEtotal_db1\n",
        "b2 = b2 - 0.5*dEtotal_db2\n",
        "\n",
        "print(\"w1 grad:\",dEtotal_dw1)\n",
        "print('The new w1 is:')\n",
        "print(w1.numpy())\n",
        "print(\"w2 grad:\",dEtotal_dw2)\n",
        "print('The new w2 is:')\n",
        "print(w2.numpy())\n",
        "print(\"b1 grad:\",dEtotal_db1)\n",
        "print('The new b1 is:')\n",
        "print(b1.numpy())\n",
        "print(\"b2 grad:\",dEtotal_db2)\n",
        "print('The new b2 is:')\n",
        "print(b2.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.14779511 0.64382386]], shape=(1, 2), dtype=float32)\n",
            "Forward path output is: [[0.01729538 0.00231123 0.9803934 ]]\n",
            "tf.Tensor([[0.14779511 0.64382386]], shape=(1, 2), dtype=float32)\n",
            "Total loss =  [6.069976]\n",
            "w1 grad: tf.Tensor(\n",
            "[[ 0.02293857 -0.0806224 ]\n",
            " [ 0.240855   -0.84653515]], shape=(2, 2), dtype=float32)\n",
            "The new w1 is:\n",
            "[[-2.5114694  -1.4596888 ]\n",
            " [ 0.47957253  0.8232676 ]]\n",
            "w2 grad: tf.Tensor(\n",
            "[[ 0.00255617 -0.14745352  0.14489736]\n",
            " [ 0.01113518 -0.64233583  0.6312007 ]], shape=(2, 3), dtype=float32)\n",
            "The new w2 is:\n",
            "[[-0.10127809  2.4737267  -2.2724488 ]\n",
            " [-1.5055676  -4.878832    3.3843997 ]]\n",
            "b1 grad: tf.Tensor([-0.5734643  2.01556  ], shape=(2,), dtype=float32)\n",
            "The new b1 is:\n",
            "[-1.313268   -0.30777997]\n",
            "b2 grad: tf.Tensor([ 0.01729538 -0.9976888   0.9803934 ], shape=(3,), dtype=float32)\n",
            "The new b2 is:\n",
            "[-0.00864769  0.4988444   0.5098033 ]\n"
          ]
        }
      ]
    }
  ]
}