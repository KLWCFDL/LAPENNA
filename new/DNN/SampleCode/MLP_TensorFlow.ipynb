{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdB6AqY__PXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534fc993-7b85-4f92-c86f-b3943c5c9824"
      },
      "source": [
        "# the code for updating the parameters in question 2\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "x = tf.constant([0.05, 0.1],shape=[1, 2],dtype=tf.float32) # sets the input [i1, i2]\n",
        "\n",
        "w1 = tf.constant([[0.15,0.25],[0.2,0.3]]) #setting the weight matrix for first fully connected layer [w1, w3];[w2, w4]\n",
        "w2 = tf.constant([[0.4,0.5],[0.45,0.55]]) #setting the weight matrix for second fc layer [w5, w7];[w6, w8]\n",
        "\n",
        "t = tf.constant([0.01,0.99]) # sets the target matrix [t1, t2]\n",
        "\n",
        "b1 = tf.constant([0.35, 0.35]) #sets the bias1.\n",
        "b2 = tf.constant([0.6, 0.6]) #sets the bias2.\n",
        "\n",
        "def forward(x,w1,w2,b1,b2):\n",
        "    sum1=tf.matmul(x,w1)+b1  \n",
        "    H = tf.nn.sigmoid(sum1)\n",
        "    sum2=tf.matmul(H,w2)+b2\n",
        "    O = tf.nn.sigmoid(sum2)\n",
        "    return O\n",
        "\n",
        "def backward(x,w1,w2,b1,b2,t):\n",
        "    with tf.GradientTape() as g:\n",
        "       g.watch([w1,w2,x,b1,b2])\n",
        "       O = forward(x, w1, w2, b1, b2)\n",
        "       Etotal = tf.losses.mse(t, O)\n",
        "       print(\"Total loss = \" + str(Etotal))\n",
        "    dEtotal_dw = g.gradient(Etotal, [w1,w2,x,b1,b2]) #calculates the gradients for each part\n",
        "    dEtotal_dw1, dEtotal_dw2, dEtotal_dx, dEtotal_db1, dEtotal_db2 = dEtotal_dw\n",
        "    return dEtotal_dw1,dEtotal_dw2,dEtotal_dx,dEtotal_db1,dEtotal_db2\n",
        "\n",
        "dEtotal_dw1,dEtotal_dw2,dEtotal_dx,dEtotal_db1,dEtotal_db2 = backward(x,w1,w2,b1,b2,t)\n",
        "\n",
        "w1 = w1 - 0.5*dEtotal_dw1 #the learning rate is set to be 0.5\n",
        "w2 = w2 - 0.5*dEtotal_dw2\n",
        "\n",
        "b1 = b1 - 0.5*dEtotal_db1\n",
        "b2 = b2 - 0.5*dEtotal_db2\n",
        "\n",
        "print('The new w1 is',w1.numpy())\n",
        "print('The new w2 is',w2.numpy())\n",
        "print('The new b1 is',b1.numpy())\n",
        "print('The new b2 is',b2.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss = tf.Tensor([0.2983711], shape=(1,), dtype=float32)\n",
            "The new w1 is [[0.14978072 0.24975115]\n",
            " [0.19956143 0.2995023 ]]\n",
            "The new w2 is [[0.3589165  0.5113013 ]\n",
            " [0.40866616 0.56137013]]\n",
            "The new b1 is [0.3456143  0.34502286]\n",
            "The new b2 is [0.53075075 0.61904913]\n"
          ]
        }
      ]
    }
  ]
}